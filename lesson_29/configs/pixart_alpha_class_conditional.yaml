# Configuration file for DDPM with a transformer backbone, from the paper
# "Scalable Diffusion Models with Transformers" (https://arxiv.org/abs/2212.09748)
diffusion:
  # Sampling section determines the size of the sampled output
  # from the model, and defines the sampling method used
  sampling:
    target: image_diffusion.sampling.AncestralSampler
    params:
      output_channels: 1
      output_spatial_size: 32
  # The noise scheduler to use with the forward diffusion process.
  noise_scheduler:
    # The number of noise scales
    num_scales: 1000
    # The schedule type
    schedule: "linear"
    # Loss type to use for noise prediction.
    loss_type: "l2"
    sampler:
      target: image_diffusion.importance_sampling.UniformSampler
      params:
        num_timesteps: 1000
  # A preprocessor to use with the context before sending to the score network.
  context_preprocessing:
      # No context to preprocess for this model
      - target: image_diffusion.conditioning.IgnoreContextAdapter
        params: {}
  # A preprocessor for input to the model.
  input_preprocessing:
    target: image_diffusion.conditioning.IgnoreInputPreprocessor
    params: {}
  # Setting for classifier free guidance.
  classifier_free_guidance:
    # Classifier-free guidance scale, where the value is >= 1.0
    classifier_free_guidance: 1.0
    # Unconditional guidance probability
    unconditional_guidance_probability: 0.2
    # The context signals to apply guidance to.
    signals: ["classes"]
    # For classifier free guidance, we need the ability to create an unconditional
    # context given the conditional context. This unconditional context needs
    # to be applied in both training and sampling, and will return a new
    # context dictionary given the original context dictionary.
    unconditional_context:
      target: image_diffusion.conditioning.UnconditionalClassesAdapter
      params:
        num_classes: 10
  dynamic_thresholding:
    enable: True
    p: 0.99
    c: 1.7
  # Defines the score network for predicting the noise parameter
  score_network:
    target: image_diffusion.score_networks.pixart.PixArtAlpha
    params:
      # The number of input channels to the model.
      input_channels: 1
      # The number of output channels to the model.
      output_channels: 1
      # The spatial size of the input to the model.
      input_spatial_size: 32
      # Spatial size of each image patch
      patch_size: 8
      # Hidden size of the transformer. This is also the dimensionality
      # of the cross attention context.
      hidden_size: 384
      # Transformer depth (number of transformer blocks)
      depth: 12
      # Number of attention heads in each transformer block
      num_heads: 6
      # Multiplier for internal dimensions of MLP in each transformer block,
      # mlp_ratio is a multiplier on top of hidden_size above.
      mlp_ratio: 4.0
      # Does the model include a learned sigma or a fixed sigma.
      is_learned_sigma: False
      # True if this is a class conditional model
      is_class_conditional: False
      # Dropout scale, using stochastic depth dropout
      drop_path: 0.0
      # The number of classes for a class conditional model.
      # Only used if is_class_conditional=True.
      num_classes: 10

      use_rel_pos: False
      lewei_scale: 1.0

      # Additional conditioning signals for the model. The projections
      # defined here will be applied before running through the rest of the
      # score network.
      conditioning:
        # The signals (keys in the dictionary) that are available in the conditioning
        # context.
        signals: ["timestep", "classes", "text_tokens", "text_prompts"]
        projections:
          # A projection to apply to the integer timesteps.
          timestep:
            # Defines a projection incorporating the sinusoidal position embedding.
            # Output size is (B, C, num_features * time_embedding_mult)
            target: image_diffusion.layers.embedding.DiTTimestepEmbedding
            params:
              hidden_size: 384
              frequency_embedding_size: 256
          classes:
            # Defines a projection incorporating the class labels
            target: image_diffusion.layers.embedding.DiTLabelEmbedding
            params:
              hidden_size: 384
              num_classes: 10
              # Drops a portion of the label embeddings (seting to zero)
              # to ignore the class embedding totally (rather than configuring them
              # for unconditional generation).
              drop_prob: 0.1
          # A projection to apply to the text tokens in the conditioning context.
          text_tokens:
            # Defines an embedding which goes from text tokens at the given
            # vocabulary size to text token embeddings.
            target: image_diffusion.layers.embedding.T5TextTokensToEmbedding
            params:
              model_name: "google/t5-v1_1-base"
          text_prompts:
            # The Prompts Preprocessor converts the list of text prompts in the context
            # into a batch of text tokens of shape (B, text_context_size)
            target: image_diffusion.layers.embedding.T5TextPromptsToTokens
            params:
              # The max length of the text token sequence
              max_length: 77
              # The name of the T5 text model to use
              model_name: "google/t5-v1_1-base"

        # The context transformer to use at the top of the score network. This transforms
        # the context with a shared set of parameters.
        context_transformer_head:
          # Timestep -> timestep embedding
          - target: image_diffusion.layers.embedding.RunProjection
            params:
              input_context_key: "timestep"
              output_context_key: "timestep_embedding"
              projection_key: "timestep"
          # Class labels -> class embeddings
          - target: image_diffusion.layers.embedding.RunProjection
            params:
              input_context_key: "classes"
              output_context_key: "class_embeddings"
              projection_key: "classes"
          # Text prompts -> text tokens
          - target: image_diffusion.layers.embedding.RunProjection
            params:
              input_context_key: "text_prompts"
              output_context_key: "text_tokens"
              projection_key: "text_prompts"
          # Text tokens -> text embeddings
          - target: image_diffusion.layers.embedding.RunProjection
            params:
              input_context_key: "text_tokens"
              output_context_key: "text_embeddings"
              projection_key: "text_tokens"
          # Combines the timestep and class embedding signals into a single embedding
          - target: image_diffusion.layers.embedding.DiTCombineEmbeddngs
            params:
              output_context_key: "timestep_embedding"
              source_context_keys: ["class_embeddings", "timestep_embedding"]
          # Projects the text embeddings into the cross attention layers.
          - target: image_diffusion.layers.embedding.ContextProjection
            params:
              input_context_key: "text_embeddings"
              output_context_key: "context_embeddings"
              in_features: 768
              hidden_features: 384
              out_features: 384
              custom_initialization: True
        # The context transformer to use at each attention layer. This transforms
        # the context with a different set of parameters at each attention layer.
        context_transformer_layer:
          target: torch.nn.Identity
          params: {}
# Describes the dataset used in training.
data:
  # Spatial width/height of the data input to the model.
  image_size: 32
  # Number of channels in the input data
  num_channels: 1
  # The number of classes in the dataset
  num_classes: 10
# Optional optimizer specification
optimizer:
  target: torch.optim.Adam
  params:
    lr: .0002
    betas: [0.9, 0.99]
