# Configuration for a DaLL*2 Diffusion Network
clip:
  dim_text: 128
  dim_image: 128
  dim_latent: 128

  num_text_tokens: 49408
  text_enc_depth: 4
  text_seq_len: 77
  text_heads: 1

  visual_enc_depth: 4
  visual_image_size: 32
  visual_image_channels: 1
  visual_patch_size: 8
  visual_heads: 1
  visual_patch_dropout: 0.5

  use_all_token_embeds: False
  decoupled_contrastive_learning: False
  extra_latent_projection: False
  use_visual_ssl: True
  use_mlm: False
  text_ssl_loss_weight: 0.05
  image_ssl_loss_weight: 0.05

diffusion_decoder:
  model:
    # The number of input channels to the model
    input_channels: 1
    # The number of output channels to the model
    output_channels: 1
    # The spatial size of the input to the model
    input_spatial_size: 32
    # The number of noise scales
    num_scales: 1000
    # The number of features/channels at the start of
    # the network.
    num_features: 128
    # Resnet block channel multipliers.
    channel_multipliers: [1, 2, 2, 2]
    # The number of resnet blocks per resolution.
    num_resnet_blocks: 2
    # The resolution to apply attention layers.
    attention_resolutions: [16]
    # The number of attention heads to use
    num_attention_heads_downsample: 4
    num_attention_heads_upsample: 4
    # Use scale/shift of the GroupNorm in the timestep embedding.
    # This is also called Adaptive Group Normalization.
    use_scale_shift_norm: True
    # Perform resampling using convolutions.
    resamp_with_conv: False
    # BigGAN style resnet block to perform up/down sampling.
    resblock_updown: False
    # The type of resnet block to use
    resnet_block_type: 'biggan'
    # Dropout scale
    dropout: 0.1
    # Is this a v or epsilon-param model?
    is_v_param: False
    # True if this is a class conditional model
    is_class_conditional: False
    # Classifier-free guidance scale, where the value is >= 1.0
    classifier_free_guidance: 1.0
    # Unconditional guidance probability
    unconditional_guidance_probability: 0.2
    # The number of heads in the attention layers
    attention_heads: 1
    # The number of channels in the attention layers
    attention_channels: 64
    # The size of the text/image context dimension, for text conditioning
    # signals.
    context_size: 768
    # The vocabulary size of the text encoder. The total
    # size of the vocabulary is much larger, but we have a much
    # smaller dictionary of tokens that doesn't use the whole vocabulary.
    text_vocabulary_size: 50257
    # The number of image tokens to project and concatenate into the
    # conditioning context.
    num_image_tokens: 4
    # Transformer for conditioning on the text context
    context_transformer:
      context: 768
      num_layers: 6
      attention_heads: 2
      attention_channels: 64
      final_layer_norm: True
      padding: False
  data:
    # Spatial width/height of the data input to the model.
    image_size: 32
    # Number of channels in the input data
    num_channels: 1
    # The number of classes in the dataset
    num_classes: 10

diffusion_prior:
  model:
    num_text_embeddings: 1
    num_image_embeddings: 1
    num_time_embeddings: 1
    num_timesteps: 1000
    transformer:
      context_size: 768
      width: 768
      num_layers: 6
      attention_heads: 1
      attention_channels: 64
      final_layer_norm: True
      padding: False

  text_encoder:
    # The total vocabulary size
    vocabulary_size: 50257
    # The number of tokens in a sequence
    tokens_in_sequence: 77

data:
  # Spatial width/height of the data input to the model.
  image_size: 32
  # Number of channels in the input data
  num_channels: 1
  # The number of classes in the dataset
  num_classes: 10

